{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1 : Use HDF CLI (from the command line or python)\n",
    "### Copy data To and From the local file system and work from there\n",
    "#### **Applicable to: \"small and medium\" datasets ; ie : small enough to be easily managed/processed locally.**  \n",
    "NOTE: To be used in the context of sensitive data and/or data that requiere strong governance as it creates a break in the data chain of custody / security.  \n",
    "This is not a best practice in terms of Data Management/Governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CML data storage\n",
    "\n",
    "There a 2 types of storage in CML  \n",
    "- **Local Storage** : linked to the projet  \n",
    "  => stores scripts and (optinally) small datasets\n",
    "- **Shared Storage** : This is the object store (S3 for AWS / ADLS for AZURE) that is linked to the Environment that CML is attached to.   \n",
    "  => default storage for datasets  \n",
    "  \n",
    "It is good practice to declare the default env bucket linked to a CML workspace in a *global* environment variable (Admin -> Engine -> Env Variable)\n",
    "In this case I used the `$STORAGE` env variable.\n",
    "Data is stored by default in the `/$STORAGE/datalake` subfolder for example:  `s3a://demo-aws-1/datalake/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ENV_BUCKET=\"s3a://demo-aws-2\"\n",
    "\n",
    "try : \n",
    "  DL_s3bucket=os.environ[\"STORAGE\"]+\"/datalake/\"\n",
    "except KeyError: \n",
    "  DL_s3bucket=ENV_BUCKET\n",
    "  os.environ[\"STORAGE\"]=ENV_BUCKET+\"/datalake/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## Cleanup - delete file if exits \n",
    "## Note : $STORAGE env variable indicates the root of the default env bucket linked to a CML workspace \n",
    "hdfs dfs -rm -f $STORAGE/datalake/tmp/1988.csv.bz2\n",
    "hdfs dfs -rm -f $STORAGE/datalake/tmp/airports.csv\n",
    "hdfs dfs -rm -f $STORAGE/datalake/tmp/carriers_python.csv\n",
    "hdfs dfs -rm -f $STORAGE/datalake/tmp/airports_pipe.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Copy data TO HDFS from local file system\n",
    "### Using the command line (works in workbench as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/airlines\n",
      "-rw-rw-rw-   1 mlamairesse mlamairesse     244438 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/airports.csv\n",
      "-rw-rw-rw-   1 mlamairesse mlamairesse      43758 2020-04-16 13:52 s3a://demo-aws-2/datalake/tmp/carriers.csv\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/models\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/wine_pred\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/wine_pred_hive\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/wine_predicted.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/04/16 15:14:23 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "20/04/16 15:14:23 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "20/04/16 15:14:23 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Apr 16, 2020 3:14:23 PM org.apache.knox.gateway.shell.KnoxSession createClient\n",
      "INFO: Using default JAAS configuration\n",
      "20/04/16 15:14:25 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "20/04/16 15:14:25 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "20/04/16 15:14:25 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n",
      "20/04/16 15:14:26 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "20/04/16 15:14:26 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "20/04/16 15:14:26 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Apr 16, 2020 3:14:26 PM org.apache.knox.gateway.shell.KnoxSession createClient\n",
      "INFO: Using default JAAS configuration\n",
      "20/04/16 15:14:28 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "20/04/16 15:14:28 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "20/04/16 15:14:28 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "### Copy to HDFS - using HDFS client - from Bash\n",
    "# NOTE: Only functional from a Bash script or an interactive session \n",
    "hdfs dfs -copyFromLocal -f /home/cdsw/airlines/airports/airports.csv $STORAGE/datalake/tmp/\n",
    "hdfs dfs -ls $STORAGE/datalake/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python - Write data to the HDFS CLI (via subprocess)Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://demo-aws-2/datalake/tmp/carriers_python.csv\n"
     ]
    }
   ],
   "source": [
    "### local and HDFS Paths \n",
    "import os \n",
    "shared_root=os.environ[\"STORAGE\"]+\"/datalake/\"\n",
    "local_path=\"/home/cdsw/airlines/carriers/carriers.csv\"\n",
    "shared_path=shared_root+\"tmp/carriers_python.csv\"\n",
    "print(shared_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/04/16 15:14:32 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "20/04/16 15:14:32 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "20/04/16 15:14:32 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Apr 16, 2020 3:14:32 PM org.apache.knox.gateway.shell.KnoxSession createClient\n",
      "INFO: Using default JAAS configuration\n",
      "Found 8 items\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/airlines\n",
      "-rw-rw-rw-   1 mlamairesse mlamairesse     244438 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/airports.csv\n",
      "-rw-rw-rw-   1 mlamairesse mlamairesse      43758 2020-04-16 13:52 s3a://demo-aws-2/datalake/tmp/carriers.csv\n",
      "-rw-rw-rw-   1 mlamairesse mlamairesse      43758 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/carriers_python.csv\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/models\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/wine_pred\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/wine_pred_hive\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/wine_predicted.parquet\n",
      "20/04/16 15:14:33 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "20/04/16 15:14:33 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "20/04/16 15:14:33 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "import sys\n",
    "\n",
    "def hdfs_write(local_path,hdfs_path):\n",
    "    ### Copy to HDFS - Python (using subprocess)\n",
    "    from subprocess import Popen, PIPE\n",
    "    put = Popen([\"hadoop\",\"fs\",\"-put\",\"-f\", local_path, hdfs_path], stdin=PIPE,stdout=PIPE,stderr=PIPE)\n",
    "    stdout, stderr = put.communicate()\n",
    "    \n",
    "    ## Error handling\n",
    "    if put.returncode != 0: \n",
    "        raise IOError(stderr)\n",
    "\n",
    "hdfs_write(local_path,shared_path)\n",
    "\n",
    "#Show hdfs path\n",
    "!hdfs dfs -ls $STORAGE/datalake/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** Use a pipe to download files directly to HDFS (command line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     020/04/16 15:14:34 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "20/04/16 15:14:34 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "20/04/16 15:14:34 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Apr 16, 2020 3:14:34 PM org.apache.knox.gateway.shell.KnoxSession createClient\n",
      "INFO: Using default JAAS configuration\n",
      "\r",
      " 31  238k   31 75896    0     0  30034      0  0:00:08  0:00:02  0:00:06 30022\r",
      "100  238k  100  238k    0     0  96730      0  0:00:02  0:00:02 --:--:-- 96692\n",
      "20/04/16 15:14:37 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "20/04/16 15:14:37 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "20/04/16 15:14:37 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## NOTE: Only functional from a Bash script or an interactive session \n",
    "export DOWNLOAD_LINK='http://stat-computing.org/dataexpo/2009/airports.csv'\n",
    "curl $DOWNLOAD_LINK | hadoop fs -put - $STORAGE/datalake/tmp/airports_pipe.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/04/16 15:14:38 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "20/04/16 15:14:38 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "20/04/16 15:14:38 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Apr 16, 2020 3:14:38 PM org.apache.knox.gateway.shell.KnoxSession createClient\n",
      "INFO: Using default JAAS configuration\n",
      "Found 9 items\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/airlines\n",
      "-rw-rw-rw-   1 mlamairesse mlamairesse     244438 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/airports.csv\n",
      "-rw-rw-rw-   1 mlamairesse mlamairesse     244438 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/airports_pipe.csv\n",
      "-rw-rw-rw-   1 mlamairesse mlamairesse      43758 2020-04-16 13:52 s3a://demo-aws-2/datalake/tmp/carriers.csv\n",
      "-rw-rw-rw-   1 mlamairesse mlamairesse      43758 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/carriers_python.csv\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/models\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/wine_pred\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/wine_pred_hive\n",
      "drwxrwxrwx   - mlamairesse mlamairesse          0 2020-04-16 15:14 s3a://demo-aws-2/datalake/tmp/wine_predicted.parquet\n",
      "20/04/16 15:14:39 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "20/04/16 15:14:39 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "20/04/16 15:14:39 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls $STORAGE/datalake/tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading / Writing files from S3 in CML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Option 1 - USE  SPARK\n",
    "### **Applicable to:**  All datasets and large ones in particular <br> \n",
    "IDBroker integration allows direct access to the S3 buckets associated with the environement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 1. Reading data from an S3 bucket associated with the Environement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the spark session\n",
    "Custom session configuration can be defined either in the session parameters as below OR\n",
    "inside a `spark-defaults.conf` file stored at the root of the project (in which case the configs become project wide)  \n",
    "**NOTE** Buckets to be accessed must be indicated in the Spark configuration `spark.yarn.access.hadoopFileSystems`  \n",
    "Below an example to access the default bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket accessed: s3a://demo-aws-2/datalake/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "shared_root=os.environ[\"STORAGE\"]+\"/datalake/\"\n",
    "print(\"Bucket accessed: \"+shared_root)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "  .builder\\\n",
    "  .appName('Airline')\\\n",
    "  .config(\"spark.executor.memory\",\"2g\")\\\n",
    "  .config(\"spark.executor.cores\",\"2\")\\\n",
    "  .config(\"spark.executor.instances\",\"3\")\\\n",
    "  .config(\"spark.yarn.access.hadoopFileSystems\",shared_root)\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://spark-ir4a4eizdri7h8aw.ml-e493d729-039.demo-aws.ylcu-atmi.cloudera.site\" target=\"_blank\" >Spark UI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Adding a link to the Spark UI for demo purposes\n",
    "## Also available in the session tab\n",
    "from IPython.core.display import HTML\n",
    "import os\n",
    "HTML('<a href=\"http://spark-{}.{}\" target=\"_blank\" >Spark UI</a>'.\\\n",
    "     format(os.getenv(\"CDSW_ENGINE_ID\"),os.getenv(\"CDSW_DOMAIN\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data - CSV file stored on HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location read: s3a://demo-aws-2/datalake/tmp/airports.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "shared_root=os.environ[\"STORAGE\"]+\"/datalake/\"\n",
    "shared_path=shared_root+'tmp/airports.csv' #bucket location\n",
    "print(\"location read: \"+shared_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- iata: string (nullable = true)\n",
      " |-- airport: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_df = spark.read.csv(\n",
    "    path=shared_path,\n",
    "    header=True,\n",
    "    sep=',',\n",
    "    inferSchema=True,\n",
    "    nullValue=None\n",
    ")\n",
    "airports_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : in the above example, I'm infering the schema from the file. <br>\n",
    "It's actually good practice to set the schema to prevent erroneous type casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------------+-----+-------+-----------+------------+\n",
      "|iata|             airport|            city|state|country|        lat|        long|\n",
      "+----+--------------------+----------------+-----+-------+-----------+------------+\n",
      "| 00M|            Thigpen |     Bay Springs|   MS|    USA|31.95376472|-89.23450472|\n",
      "| 00R|Livingston Municipal|      Livingston|   TX|    USA|30.68586111|-95.01792778|\n",
      "| 00V|         Meadow Lake|Colorado Springs|   CO|    USA|38.94574889|-104.5698933|\n",
      "| 01G|        Perry-Warsaw|           Perry|   NY|    USA|42.74134667|-78.05208056|\n",
      "| 01J|    Hilliard Airpark|        Hilliard|   FL|    USA| 30.6880125|-81.90594389|\n",
      "+----+--------------------+----------------+-----+-------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField(\"iata\", StringType(), True),\n",
    "                     StructField(\"airport\", StringType(), True),\n",
    "                     StructField(\"city\", StringType(), True),\n",
    "                     StructField(\"state\", StringType(), True),\n",
    "                     StructField(\"country\", StringType(), True),\n",
    "                     StructField(\"lat\",  DoubleType(), True),\n",
    "                     StructField(\"long\",  DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "airports_df = spark.read.csv(\n",
    "    path=shared_path,\n",
    "    schema=schema,\n",
    "    header=True,\n",
    "    sep=',',\n",
    "    nullValue=None\n",
    ").cache()\n",
    "airports_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Transform data to Pandas Dataframe\n",
    "#### Once converted **ALL DATA will be brought locally** and distributed processing ends \n",
    "* **Applicable to : SMALL to MEDIUM size datasets** - ie : datasets that can be easily managed/processed locally\n",
    "* When working with **LARGE datasets** :  **data should be sampled** before bringing it locally\n",
    "\n",
    "> **Good Practice**:  Spark context should be stopped `spark.stop()` to release cluster ressources once data is copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3376 entries, 0 to 3375\n",
      "Data columns (total 7 columns):\n",
      "iata       3376 non-null object\n",
      "airport    3376 non-null object\n",
      "city       3376 non-null object\n",
      "state      3376 non-null object\n",
      "country    3376 non-null object\n",
      "lat        3376 non-null float64\n",
      "long       3376 non-null float64\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 184.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#without sampling\n",
    "import pandas \n",
    "airport_pandas_df = airports_df.toPandas()\n",
    "airport_pandas_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1111 entries, 0 to 1110\n",
      "Data columns (total 7 columns):\n",
      "iata       1111 non-null object\n",
      "airport    1111 non-null object\n",
      "city       1111 non-null object\n",
      "state      1111 non-null object\n",
      "country    1111 non-null object\n",
      "lat        1111 non-null float64\n",
      "long       1111 non-null float64\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 60.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#with sampling\n",
    "sample_pandas_df = airports_df.sample(1/3,seed=30).toPandas()\n",
    "sample_pandas_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write data from PANDAS to HDFS - Using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata</th>\n",
       "      <th>airport</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>ADK</td>\n",
       "      <td>Adak</td>\n",
       "      <td>Adak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>51.877964</td>\n",
       "      <td>-176.646031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>AKK</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>56.938691</td>\n",
       "      <td>-154.182556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>Z13</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904532</td>\n",
       "      <td>-161.420910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>AKI</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904812</td>\n",
       "      <td>-161.227019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>KQA</td>\n",
       "      <td>Akutan SPB</td>\n",
       "      <td>Akutan</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>54.132467</td>\n",
       "      <td>-165.785311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     iata     airport      city state country        lat        long\n",
       "776   ADK        Adak      Adak    AK     USA  51.877964 -176.646031\n",
       "818   AKK      Akhiok    Akhiok    AK     USA  56.938691 -154.182556\n",
       "3363  Z13    Akiachak  Akiachak    AK     USA  60.904532 -161.420910\n",
       "817   AKI       Akiak     Akiak    AK     USA  60.904812 -161.227019\n",
       "1994  KQA  Akutan SPB    Akutan    AK     USA  54.132467 -165.785311"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read from pandas\n",
    "import pandas as pd\n",
    "airlines_pd_df = pd.read_csv(\"/home/cdsw/airlines/airports/airports.csv\",sep=',', delimiter=None, header='infer')\n",
    "airlines_pd_df.sort_values(by=['state','airport'],inplace=True) # ordering to keep same visulisation order \n",
    "airlines_pd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Pandas DataFrame to Spark DataFrame\n",
    "With spark 2.3 and up, integration with Pandas has been reinforced notably with the use of Arrow for faster data transfers [https://issues.apache.org/jira/browse/SPARK-20791]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+-----+-------+-----------+-------------------+\n",
      "|iata|   airport|    city|state|country|        lat|               long|\n",
      "+----+----------+--------+-----+-------+-----------+-------------------+\n",
      "| ADK|      Adak|    Adak|   AK|    USA|51.87796389|       -176.6460306|\n",
      "| AKK|    Akhiok|  Akhiok|   AK|    USA|56.93869083|       -154.1825556|\n",
      "| Z13|  Akiachak|Akiachak|   AK|    USA|60.90453167|-161.42091000000002|\n",
      "| AKI|     Akiak|   Akiak|   AK|    USA|60.90481194|       -161.2270189|\n",
      "| KQA|Akutan SPB|  Akutan|   AK|    USA|54.13246694|       -165.7853111|\n",
      "+----+----------+--------+-----+-------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (optional) Enable Arrow-based optimised columnar data transfers ; Note : still marked as experimental\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") # Note : Compatible only with pyarrow 0.8.0 \n",
    "# https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#ensure-pyarrow-installed\n",
    "\n",
    "#(optional) good practice to define schema to prevent any type casting errors\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField(\"iata\", StringType(), True),\n",
    "                     StructField(\"airport\", StringType(), True),\n",
    "                     StructField(\"city\", StringType(), True),\n",
    "                     StructField(\"state\", StringType(), True),\n",
    "                     StructField(\"country\", StringType(), True),\n",
    "                     StructField(\"lat\",  DoubleType(), True),\n",
    "                     StructField(\"long\",  DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "spark_df=spark.createDataFrame(airlines_pd_df,schema=schema)\n",
    "spark_df.orderBy(['state','airport']).show(5) # ordering to keep same visulisation order "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to HDFS - using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location read: s3a://demo-aws-2/datalake//tmp/airlines/spark_write\n",
      "root\n",
      " |-- iata: string (nullable = true)\n",
      " |-- airport: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## It's good practice to restructure data before writing to HDFS : Spark write a file by partition. \n",
    "## this can lead to lots of small files which is counterproductive both for read and write. \n",
    "## Re-organize data using the \"coalesce\" function to define the number of files to be saved\n",
    "import os\n",
    "shared_root=os.environ[\"STORAGE\"]+\"/datalake/\"\n",
    "location = shared_root+'/tmp/airlines/spark_write'\n",
    "print(\"location read: \"+location)\n",
    "\n",
    "spark_df.coalesce(2).write.parquet(location, mode='overwrite')\n",
    "\n",
    "test_file_df = spark.read.parquet(location)\n",
    "test_file_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data to Hive - using Spark\n",
    "Spark to hive integration makes it very easy to interact with the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note : Ordering on write can help optimise reads later on. \n",
    "spark_df.orderBy(['state','airport']).coalesce(2)\\\n",
    "    .write.format('parquet').mode(\"overwrite\")\\\n",
    "    .saveAsTable('flights.airports_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read Data from Hive \n",
    "All hive configurations are already injected into spark.  Therefore Hive can be called directly using a spark sql context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----------+\n",
      "|database|   tableName|isTemporary|\n",
      "+--------+------------+-----------+\n",
      "| flights|    airports|      false|\n",
      "| flights|airports_new|      false|\n",
      "| flights|    carriers|      false|\n",
      "| flights| flights_raw|      false|\n",
      "+--------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_statement = '''show tables in flights'''\n",
    "spark.sql(sql_statement).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------------+-----+-------+-----------+------------+\n",
      "|iata|   airport|          city|state|country|        lat|        long|\n",
      "+----+----------+--------------+-----+-------+-----------+------------+\n",
      "| ADK|      Adak|          Adak|   AK|    USA|51.87796389|-176.6460306|\n",
      "| AKK|    Akhiok|        Akhiok|   AK|    USA|56.93869083|-154.1825556|\n",
      "| Z13|  Akiachak|      Akiachak|   AK|    USA|60.90453167|  -161.42091|\n",
      "| AKI|     Akiak|         Akiak|   AK|    USA|60.90481194|-161.2270189|\n",
      "| KQA|Akutan SPB|        Akutan|   AK|    USA|54.13246694|-165.7853111|\n",
      "| AUK|  Alakanuk|      Alakanuk|   AK|    USA|62.68004417|-164.6599253|\n",
      "| 5A8| Aleknagik|     Aleknagik|   AK|    USA|59.28256167|-158.6176725|\n",
      "| 6A8| Allakaket|     Allakaket|   AK|    USA|66.55194444|-152.6222222|\n",
      "| BIG| Allen AAF|Delta Junction|   AK|    USA|63.99454722|-145.7216417|\n",
      "| AFM|    Ambler|        Ambler|   AK|    USA|67.10610472|-157.8536203|\n",
      "+----+----------+--------------+-----+-------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read table\n",
    "sql_statement = '''select * from flights.airports where state = \"AK\" '''\n",
    "airports_df = spark.sql(sql_statement)\n",
    "airports_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata</th>\n",
       "      <th>airport</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADK</td>\n",
       "      <td>Adak</td>\n",
       "      <td>Adak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>51.877964</td>\n",
       "      <td>-176.646031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AKK</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>56.938691</td>\n",
       "      <td>-154.182556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z13</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904532</td>\n",
       "      <td>-161.420910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AKI</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904812</td>\n",
       "      <td>-161.227019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KQA</td>\n",
       "      <td>Akutan SPB</td>\n",
       "      <td>Akutan</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>54.132467</td>\n",
       "      <td>-165.785311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iata     airport      city state country        lat        long\n",
       "0  ADK        Adak      Adak    AK     USA  51.877964 -176.646031\n",
       "1  AKK      Akhiok    Akhiok    AK     USA  56.938691 -154.182556\n",
       "2  Z13    Akiachak  Akiachak    AK     USA  60.904532 -161.420910\n",
       "3  AKI       Akiak     Akiak    AK     USA  60.904812 -161.227019\n",
       "4  KQA  Akutan SPB    Akutan    AK     USA  54.132467 -165.785311"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(OPTIONAL) convert to pandas \n",
    "airlines_pd_df = airports_df.toPandas()\n",
    "airlines_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() ## Release spark ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***NOTE:*** Pandas Dataframe is still available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata</th>\n",
       "      <th>airport</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00M</td>\n",
       "      <td>Thigpen</td>\n",
       "      <td>Bay Springs</td>\n",
       "      <td>MS</td>\n",
       "      <td>USA</td>\n",
       "      <td>31.953765</td>\n",
       "      <td>-89.234505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00R</td>\n",
       "      <td>Livingston Municipal</td>\n",
       "      <td>Livingston</td>\n",
       "      <td>TX</td>\n",
       "      <td>USA</td>\n",
       "      <td>30.685861</td>\n",
       "      <td>-95.017928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00V</td>\n",
       "      <td>Meadow Lake</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>USA</td>\n",
       "      <td>38.945749</td>\n",
       "      <td>-104.569893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01G</td>\n",
       "      <td>Perry-Warsaw</td>\n",
       "      <td>Perry</td>\n",
       "      <td>NY</td>\n",
       "      <td>USA</td>\n",
       "      <td>42.741347</td>\n",
       "      <td>-78.052081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01J</td>\n",
       "      <td>Hilliard Airpark</td>\n",
       "      <td>Hilliard</td>\n",
       "      <td>FL</td>\n",
       "      <td>USA</td>\n",
       "      <td>30.688012</td>\n",
       "      <td>-81.905944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iata               airport              city state country        lat  \\\n",
       "0  00M              Thigpen        Bay Springs    MS     USA  31.953765   \n",
       "1  00R  Livingston Municipal        Livingston    TX     USA  30.685861   \n",
       "2  00V           Meadow Lake  Colorado Springs    CO     USA  38.945749   \n",
       "3  01G          Perry-Warsaw             Perry    NY     USA  42.741347   \n",
       "4  01J      Hilliard Airpark          Hilliard    FL     USA  30.688012   \n",
       "\n",
       "         long  \n",
       "0  -89.234505  \n",
       "1  -95.017928  \n",
       "2 -104.569893  \n",
       "3  -78.052081  \n",
       "4  -81.905944  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTION 2 - Read_Write Directly from Pandas ( using AWS secret and key ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas can read directly from S3\n",
    "Dependencies : s3fs library\n",
    "> Note : AWS CLI must be configured ahead of time with AWS key and secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install s3fs==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Note : best practice is not to set AWS access and secret in config files or via command line, not in script\n",
    "\n",
    "export AWS_ACCESS_KEY_ID=XXXXX\n",
    "export AWS_SECRET_ACCESS_KEY=XXXXX\n",
    "export AWS_DEFAULT_REGION=us-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "shared_root=os.environ[\"STORAGE\"]+\"/datalake/\"\n",
    "shared_path=shared_root+'tmp/airports.csv' #bucket location\n",
    "print(\"location read: \"+shared_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_pd_df = pd.read_csv(shared_path,sep=';', delimiter=None, header='infer')\n",
    "airlines_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
